* Flights

weaves

Jnygre.Q.Rnirf@tznvy.pbz

* Overview

This is the Weak legs of flights test dataset under analysis for Strong and
Weak legs of multi-haul flights by an airline operator.

I've used R, the caret package and the model I chose to use was gbm or Gradient
Boosting Method. 

 https://cran.r-project.org/web/packages/gbm/gbm.pdf

I've used this combination before successfully. It handles mixed data-sets
(ordinal and interval) quite well without much extra effort. 

The Weak legs are aberrations and, as is known, these can be very difficult
to detect.

To summarise the steps of my approach, 

 - I undertook some preliminary data analysis with Excel documented
   elsewhere.

 - After that,  I was  able to  drop some variables  from the  original CSV
   data-set because of duplication - and derivation issues, especially D00,
   this was a deterministic Probit predictor used to classify the outcome
   variable LEGTYPE into Weak or Strong.

 - I made an important change regarding SDEPHR and SARRHR, I decided they
   were a time series and could be related by a Duration metric xDURN2 that
   I introduced.

 - I reduced the dataset to the range that was requested in the results
   based on AVGLOFATC. Instead of being just over 1300, it became around
   300.

 - I made a number of preliminary investigations to check correlations on
   numeric variables to see if were dominant. Five or so variables were
   dropped because of correlations. There were no zero or near-zero
   variance variables. I also imputed some values for a numeric variable
   that had been grouped.

 - I then validated the original LEGTYPE at the 80th percentile (derived from
   the Probit) and got a perfect result from predictor the training set and
   a pretty good result from the test set.

 - I then revised the LEGTYPE defintion to operate at about
   45%. Critically, I used the same Probit function which I reconstructed
   as an empirical CDF. This revised many previously Strong legs as Weak.

 - I then revised my model implementation to produce some plausible results
   for this data-set. 

 - I have only suggested a course of approach for the final part of the
   exercise. A seasonal adjustment factor can be derived from linear
   regression of the observed errors.

** Variables Dropped

These were dropped because they were observed to be they were observed to
duplicated information or were originators and derived other metrics.

DEPGRP and ARRGRP
The RANK variables 
D00 was generated by the Probit (and was thus used to generate LEGTYPE)
D80THPCTL was a constant used with D00

These were dropped as because they were too highly correlated (>0.65) to
another variable and would have introduced bias and would lead to
overfitting.

[1] "SDEPHR"       "DEPBUCKET"    "ARRSPOKE"     "DEPSPOKE"     "AVGSQ"       
[6] "TRNRANKGRP"   "xAVAILBUCKET"

Of these, some were introduced by me, these were

xAVAILBUCKET was a numeric implementation of the AVAILBUCKET.  ARRSPOKE and
DEPSPOKE were introduced by me to homogenize SKDARRSTA and SDDEPSTA and
remove DEPGRP and ARRGRP.

** Samples Dropped

I dropped all the samples where the AVGLOFATC was not in [3.1, 4.4]. The
predictor was only to be used within that range.

** Results

These results are for the revised estimator that was designed to achieve a 40% error
detection rate.

*** Results: LEGTYPE from 55th percentile

I spent some time tuning this and got 85% accuracy on the training set.

          Reference
Prediction Strong Weak
    Strong     76   14
    Weak       17  102
                                          
               Accuracy : 0.8517          

For a test data set, I got a respectable result.

          Reference
Prediction Strong Weak
    Strong     31   22
    Weak       19   39
                                          
               Accuracy : 0.6306          

*** Validation: LEGTYPE from 80th percentile

I validated my model choice by using the 80th percentile data generated
with the outcome (Weak/Strong for LEGTYPE) from the Probit. And, of
course, I did this first.

The training set is a perfect detector. (This should be the norm, but is
actually quite a rare occurrence.) And, do note, this did not make use of
D00.

          Reference
Prediction Strong Weak
    Strong    180    0
    Weak        0   61

               Accuracy : 1          

A test set then reported

          Reference
Prediction Strong Weak
    Strong     46   12
    Weak       13    8
                                          
               Accuracy : 0.6835          

Not a very good estimator at all. I could have put more work into tuning
the model, but I thought I had better press on with the main part of the
exercise: the "Results: LEGTYPE from 55th percentile" above.

* Method

** Data Analysis

The early Excel data analysis is described in the PowerPoint
presentation. I made a key innovation when working with R that Arrival Time
is always after Departure Time, so I concluded that, even though the
results were aggregated, it was valid to deduce a flight duration from the
difference between the too.

Early data analysis also showed the classification of the D00 and assured
me that D00 was a Probit classifier.

** Model choice

As mentioned above, I chose to use GBM, but via the caret R package, and,
also above, that it handles mixed data-types comfortably, has a consistent
interface for scaling, data validation and imputation.

GBM uses classification trees transparently so there is less to configure.

** Code and Logs

I must mention that the model runs from the file flight0.R. And the same
file was used for both the original and revised model estimation. Some of
the key parameters have been changed, but these can be viewed in the log
files.

results0.log is for the original 80th percentile model - the validation phase.

flight1.R was introduced to estimate the Probit (using an empirical
distribution - I only used an empirical distribution and did not perform
any regression to fit a probit distribution.)

flight1.R was also used to adjust the LEGTYPE for a 55% percentile
detection (ie. 45% detection.)

The associated log file is results1.log.

I also keep a notes file flight-nb.R where I prototype code and keep the plots.

** Model calibration - LEGTYPE "Weak" at 80th percentile - results0.log

This part of the exercise was to validate that my model choice, and it's
implementation, could replicate the Probit (D00) in its derivation of
LEGTYPE. My goal was to get a perfect ROC - a machine learning algorithm
should be able to replicate any deterministic regression method.

*** Training 

This was the perfect result, which given the methodology was to be
expected. The principal results are above, but there is more detail here.

> confusionMatrix(trainPred, trainClass, positive = "Weak")
Confusion Matrix and Statistics

          Reference
Prediction Strong Weak
    Strong    180    0
    Weak        0   61
                                     
               Accuracy : 1          
                 95% CI : (0.9848, 1)
    No Information Rate : 0.7469     
    P-Value [Acc > NIR] : < 2.2e-16  
                                     
                  Kappa : 1          
 Mcnemar's Test P-Value : NA         
                                     
            Sensitivity : 1.0000     
            Specificity : 1.0000     
         Pos Pred Value : 1.0000     
         Neg Pred Value : 1.0000     
             Prevalence : 0.2531     
         Detection Rate : 0.2531     
   Detection Prevalence : 0.2531     
      Balanced Accuracy : 1.0000     
                                     
       'Positive' Class : Weak       

*** Test

On a test data set, the result was not so good, only 40% sensitivity, but
specificity is good at 77%. Accuracy of 68% does indicate that is a
plausible predictor.

As noted above, I chose not to spend much time tuning the model with this
data. (Model tuning is very, very data specific.) And the perfect result on
the training data was all the assurance I needed that my model choice would
be valid.

> confusionMatrix(testPred, testClass, positive = "Weak")
Confusion Matrix and Statistics

          Reference
Prediction Strong Weak
    Strong     46   12
    Weak       13    8
                                          
               Accuracy : 0.6835          
                 95% CI : (0.5692, 0.7837)
    No Information Rate : 0.7468          
    P-Value [Acc > NIR] : 0.92            
                                          
                  Kappa : 0.1767          
 Mcnemar's Test P-Value : 1.00            
                                          
            Sensitivity : 0.4000          
            Specificity : 0.7797          
         Pos Pred Value : 0.3810          
         Neg Pred Value : 0.7931          
             Prevalence : 0.2532          
         Detection Rate : 0.1013          
   Detection Prevalence : 0.2658          
      Balanced Accuracy : 0.5898          
                                          
       'Positive' Class : Weak            

** Revised model (1) results1.log

The LEGTYPE was then re-classified in the somewhat clumsy manner described
in the flight1.R. (It's clumsiness is mine, rather than recreate the Probit
using regression - I didn't know if I had the same input data for it - I
simply created an empirical distribution and read of the 55% percentile.)

As a methodology it is perfectly valid and makes excellent business
sense. Rather than collect new data and qualitatively define whether it is
Weak or Strong, use an existing model to train a predictor. The model can
then improve as new samples arrive.

So these results are for the first iteration of the model implementation and all the key
parameters are as before.

*** Training

This was quite informative: the training data does *not* have an ideal
accuracy. This would suggest that either the Probit was designed with
different data (that might have been the whole dataset and not the
restricted sample set I used) or, that there was a mistake in my model
choice and implementation. Nonetheless, the results were good enough to
proceed with GBM as a model and, given that a 45% detection rate is
within a standard deviation, errors are inevitable.

Confusion Matrix and Statistics

          Reference
Prediction Strong Weak
    Strong     77   16
    Weak       31  117
                                         
               Accuracy : 0.805          
                 95% CI : (0.7492, 0.853)
    No Information Rate : 0.5519         
    P-Value [Acc > NIR] : < 2e-16        
                                         
                  Kappa : 0.6005         
 Mcnemar's Test P-Value : 0.04114        
                                         
            Sensitivity : 0.8797         
            Specificity : 0.7130         
         Pos Pred Value : 0.7905         
         Neg Pred Value : 0.8280         
             Prevalence : 0.5519         
         Detection Rate : 0.4855         
   Detection Prevalence : 0.6141         
      Balanced Accuracy : 0.7963         
                                         
       'Positive' Class : Weak           

*** Test

The initial test data-set results are very poor, mis-classification of true
Weak as Strong very marked.

There is a trellis chart of the Receiver Operating Characteristic metrics
and these dove downwards quickly. I used airports, aircraft and flight
duration as the tree classifiers these worked well for the validation, but
the ROC metrics did degrade much more quickly for the flight duration and
aircraft than for the airports. (There are charts that can be viewed.)

Confusion Matrix and Statistics

          Reference
Prediction Strong Weak
    Strong     18   30
    Weak       17   14
                                         
               Accuracy : 0.4051         
                 95% CI : (0.296, 0.5215)
    No Information Rate : 0.557          
    P-Value [Acc > NIR] : 0.99761        
                                         
                  Kappa : -0.1614        
 Mcnemar's Test P-Value : 0.08005        
                                         
            Sensitivity : 0.3182         
            Specificity : 0.5143         
         Pos Pred Value : 0.4516         
         Neg Pred Value : 0.3750         
             Prevalence : 0.5570         
         Detection Rate : 0.1772         
   Detection Prevalence : 0.3924         
      Balanced Accuracy : 0.4162         
                                         
       'Positive' Class : Weak

** Revision (2) - results1.log

Because the training result was so poor, counter-intuitively, one should
decrease the correlation cut-off - and remove more variables - in the hope
that more predictive features will be able to emerge. So the cutoff was
reduced to 0.65  this dropped the following variables:

ARRSPOKE, SDEPHR, DEPBUCKET, ARRBUCKET, TRNRANKGRP, xAVAILBUCKET 

and accuracy improves, but only marginally for both the training set and
the test, just 3%.

I then increased the tree size and tried other variables for initial tree
branching. 

Increasing tree size did improve the testing dataset response to 63%
accuracy, but again the training set only marginally (another 3%). This
might suggest I got was lucky with the test partition. The trellis is still
diving very quickly. But after more runs, I was assured that the tree size
did appear to have helped.

The xDURN2 did prove to be a key predictor - and, as it was my own
innovation, I'm pleased with that. Other innovations did not fare well,
xAVGSKDAVAIL amongst others have tended to degrade the training set - never
a good sign.

This is my best reproducible confusion matrix for the revised
data-set. Unfortunately, I wasn't assiduous enough with my Git version
control and made some minor changes to the code (tree and correlation
cutoff) that have meant that the best test results, see below, have not
been reproducible.

**** Reproducible

These results should be, more or less, reproducible from the scripts.

This is for a test data-set. Just coming up to the 55% accuracy with good
sensitivity, but poor specificity.

> confusionMatrix(testPred, testClass, positive = "Weak")
Confusion Matrix and Statistics

          Reference
Prediction Strong Weak
    Strong     22   22
    Weak       28   39
                                          
               Accuracy : 0.5495          
                 95% CI : (0.4522, 0.6441)
    No Information Rate : 0.5495          
    P-Value [Acc > NIR] : 0.5392          
                                          
                  Kappa : 0.0802          
 Mcnemar's Test P-Value : 0.4795          
                                          
            Sensitivity : 0.6393          
            Specificity : 0.4400          
         Pos Pred Value : 0.5821          
         Neg Pred Value : 0.5000          
             Prevalence : 0.5495          
         Detection Rate : 0.3514          
   Detection Prevalence : 0.6036          
      Balanced Accuracy : 0.5397          
                                          
       'Positive' Class : Weak           

The training set has improved to 85% accuracy.

> confusionMatrix(trainPred, trainClass, positive = "Weak")
Confusion Matrix and Statistics

          Reference
Prediction Strong Weak
    Strong     76   14
    Weak       17  102
                                          
               Accuracy : 0.8517          
                 95% CI : (0.7961, 0.8969)
    No Information Rate : 0.555           
    P-Value [Acc > NIR] : <2e-16          
                                          
                  Kappa : 0.6987          
 Mcnemar's Test P-Value : 0.7194          
                                          
            Sensitivity : 0.8793          
            Specificity : 0.8172          
         Pos Pred Value : 0.8571          
         Neg Pred Value : 0.8444          
             Prevalence : 0.5550          
         Detection Rate : 0.4880          
   Detection Prevalence : 0.5694          
      Balanced Accuracy : 0.8483          
                                          
       'Positive' Class : Weak            
                                          
Warning messages:
1: In eval(expr, envir, enclos) : adjusting
2: In eval(expr, envir, enclos) :
  overfitting: correlations: err.trainDescr: SDEPHR, DEPBUCKET, ARRSPOKE, DEPSPOKE, AVGSQ, TRNRANKGRP, xAVAILBUCKET

**** Best observed (but lost)

If you do read the code for flight0.R you will see that I introduced some
code (see the tr.icols R object calculations) to lookup the indices of the
variables to be used in the initial tree branching. Initially, I had
hard-coded these indices, but, because of the changes in the correlation
cut-off, some variables were displaced and I got a very good result that I
didn't commit to Git.

I've been unable to reproduce this best result. I suspect it was a
fortunate partitioning of the training and test data, but I did look very
hard for a parametric change.

> confusionMatrix(testPred, testClass, positive = "Weak")
Confusion Matrix and Statistics

          Reference
Prediction Strong Weak
    Strong     31   22
    Weak       19   39
                                          
               Accuracy : 0.6306          
                 95% CI : (0.5338, 0.7203)
    No Information Rate : 0.5495          
    P-Value [Acc > NIR] : 0.05167         
                                          
                  Kappa : 0.2579          
 Mcnemar's Test P-Value : 0.75478         
                                          
            Sensitivity : 0.6393          
            Specificity : 0.6200          
         Pos Pred Value : 0.6724          
         Neg Pred Value : 0.5849          
             Prevalence : 0.5495          
         Detection Rate : 0.3514          
   Detection Prevalence : 0.5225          
      Balanced Accuracy : 0.6297          
                                          
       'Positive' Class : Weak            
                                          
> confusionMatrix(trainPred, trainClass, positive = "Weak")
Confusion Matrix and Statistics

          Reference
Prediction Strong Weak
    Strong     75   16
    Weak       18  100
                                          
               Accuracy : 0.8373          
                 95% CI : (0.7802, 0.8846)
    No Information Rate : 0.555           
    P-Value [Acc > NIR] : <2e-16          
                                          
                  Kappa : 0.6699          
 Mcnemar's Test P-Value : 0.8638          
                                          
            Sensitivity : 0.8621          
            Specificity : 0.8065          
         Pos Pred Value : 0.8475          
         Neg Pred Value : 0.8242          
             Prevalence : 0.5550          
         Detection Rate : 0.4785          
   Detection Prevalence : 0.5646          
      Balanced Accuracy : 0.8343          
                                          
       'Positive' Class : Weak            

** Notes

*** Reproduciblity

I've re-run quite a number of times and it works well.

That said, I've haven't kept good track of my use of set.seed() which is
critical when reproducing results.

*** Things I would have liked to have known

ATC I suspect is Air Traffic Control. It would have been useful to have
been able to apply a more direct classification approach. I had a look at
a fully classified model, see dummyVars in flight0R, but it was too large
for me to work with.

I would have liked to have understood the metrics better. DEPSTAATCIMP for
example might mean departure airport air-traffic control importance, but
I'm only guessing.

The relationship I deduced between SKDDEPSTA and SKDARRSTA and SARRHR and
SDEPHR to give a flight duration proved to be very important. Time
adjustments for local timezones are notoriously error-prone. I hope I
guessed that they were all UTC - or "Zulu time" as they call it in the Royal
Logistics Corps.

I'm still uncertain about AVGLOFATC, I really do hope it is the average
number of legs in a line of flight, as known to local air traffic control:
because I've used it to partition the samples. I felt justified when the
validation training set gave an ideal response.

The really difficult issue was the airports and aircraft. These proved to
be the most significant branching dictators for the grid. It might have
been useful to know their geographic location of airports. It would have
been useful to have known more about the aircraft's maximum flight time -
that might have helped to make sense of flight duration.

*** Things I would have liked to have done

It was partly my lack of knowledge about the many variables that I chose to
use the GBM. Had I been able to classify better, I could have used
partitioned algorithms, there may be different ATC regulations in different
airports. It is a lot to hope that a machine learning algorithm can
differentiate all of that information easily with aggregated data.

It would also have been interesting to use a Bayesian Design of Experiments
methodology to improve the model. This makes recommendations where and what
data should be collected to reduce variance.

*** Software and Hardware

I used R 3.5.2 on a Debian Stretch (Testing) VM with four cores. VMware on
Windows 7 managed to keep my editing X server, and me, active whilst the
training was done on all four cores.

Each training run took about 2 minutes using the doMC package from R that
caret; it and its many slave packages have taken the time and trouble to
deploy. It was very had to compare and tune machine learning models until
caret unified them all.

*** Very interesting business domain

When I next drift through an airport, I'll be more sympathetic to flight
delays. The engineering and co-ordination needed to overcome the logistic
difficulties of the airline industry have made me more appreciative of what
a fantastic world we live in.

*** Thank you!

This was a really interesting exercise. I rarely get the chance to work
with a well-curated dataset from beginning to end. I only wish I'd had more
time to try other models with it. I hope you won't mind if I continue to
work on it as a test data-set. I won't be publishing, but I would like to
share the test data with my colleagues.

Whatever the outcome, I've gained a great deal. Thank you again.

** This file's Emacs file variables

[  Local Variables: ]
[  mode:text ]
[  mode:outline-minor ]
[  mode:auto-fill ]
[  fill-column: 75 ]
[  coding: iso-8859-1-unix ]
[  comment-column:50 ]
[  comment-start: "[  "  ]
[  comment-end:"]" ]
[  End: ]
